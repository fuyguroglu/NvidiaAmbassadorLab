# RAG Basics: Flexible Q&A System

A **Retrieval-Augmented Generation (RAG)** system with multiple backend options, designed to work on any hardware - from basic laptops to powerful workstations!

## ğŸ¯ What is RAG?

**RAG (Retrieval-Augmented Generation)** combines document retrieval with LLM generation to answer questions based on your own documents:

1. **Retrieve** relevant information from your documents using semantic search
2. **Augment** the LLM prompt with retrieved context
3. **Generate** answers grounded in your specific documents

Unlike traditional chatbots that rely only on training data, RAG systems answer questions about **YOUR** documents without requiring model fine-tuning!

## âœ¨ Key Features

- ğŸ›ï¸ **Multiple Backend Options** - From retrieval-only (works everywhere!) to local LLMs and cloud APIs
- ğŸŒ **Beautiful Web Interface** - User-friendly Gradio UI, no command-line needed
- ğŸ’» **Hardware Flexible** - Works on laptops with 4GB RAM to workstations with GPUs
- ğŸ“š **Multi-format Support** - PDF and TXT documents
- ğŸš€ **Easy Setup** - Conda environment + pip install, ready in minutes
- ğŸ”§ **Configurable** - Adjust chunk size, retrieval count, and backends

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Your Question                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Embedding Model (Sentence Transformers)          â”‚
â”‚         Converts question to vector embedding            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Vector Database (ChromaDB) Search               â”‚
â”‚      Find most similar document chunks (Top-K)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Retrieved Context + Question                   â”‚
â”‚                        â†“                                 â”‚
â”‚              Language Model (Optional)                   â”‚
â”‚      (TinyLlama / Phi / Groq / OpenAI / etc.)           â”‚
â”‚                        â†“                                 â”‚
â”‚                  Generated Answer                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
1-rag-basics/
â”œâ”€â”€ config.py                   # Backend configurations
â”œâ”€â”€ rag_flexible.py             # Main RAG implementation
â”œâ”€â”€ app_simple.py               # Gradio web interface
â”œâ”€â”€ test_system.py              # Test with retrieval-only mode
â”œâ”€â”€ test_with_llm.py            # Test with LLM backend
â”œâ”€â”€ start_web_interface.sh      # Linux launcher script
â”œâ”€â”€ start_web_interface.bat     # Windows launcher script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ SETUP_GUIDE.md             # Detailed setup instructions
â”œâ”€â”€ CLAUDE.md                  # Development notes
â””â”€â”€ data/                      # Your documents go here!
```

## ğŸš€ Quick Start

### 1. Setup Environment

See [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed instructions for Windows and Linux!

**Quick version:**

```bash
# Create conda environment
conda create -n nvidia_rag python=3.11 -y
conda activate nvidia_rag

# Install dependencies
pip install -r requirements.txt
```

### 2. Add Your Documents

```bash
# Place your PDF or TXT files in the data folder
cp your-document.pdf data/
```

### 3. Launch Web Interface

**Linux/WSL:**
```bash
./start_web_interface.sh
```

**Windows:**
```
start_web_interface.bat
```

**Or manually:**
```bash
python app_simple.py
```

Then open **http://localhost:7860** in your browser!

## ğŸ® Using the Web Interface

1. **Select Backend** - Choose based on your hardware:
   - **Retrieval Only** - Works on ANY computer (< 1GB RAM)
   - **TinyLlama** - Small local model (3-4GB RAM)
   - **Phi-2/Phi-3** - Better local models (5-6GB RAM)
   - **Groq** - Fast cloud API (Free tier available!)
   - **OpenAI/Anthropic** - Premium cloud APIs

2. **Click "ğŸš€ Initialize System"** - Wait for setup (downloads model on first run)

3. **Ask Questions** - Type your questions and get answers with source citations!

4. **View System Info** - Click "â„¹ï¸ Show System Info" to see configuration

## ğŸ›ï¸ Backend Options

### Retrieval-Only Mode (Recommended for Beginners)

- **RAM**: < 1GB
- **Setup**: None needed
- **Speed**: Very fast
- **Output**: Shows relevant document chunks without generation

Perfect for:
- Learning how RAG retrieval works
- Testing on any computer
- Understanding document chunking

### Local LLM Models

#### TinyLlama
- **RAM**: ~3-4GB
- **Download**: ~2.2GB (first time only)
- **Speed**: Fast on GPU, acceptable on CPU
- **Quality**: Basic but functional

#### Phi-2 / Phi-3 Mini
- **RAM**: ~5-6GB
- **Download**: ~5GB (first time only)
- **Speed**: Medium
- **Quality**: Good instruction following

### Cloud API Models

#### Groq (Recommended!)
- **RAM**: < 1GB (runs in cloud)
- **Cost**: Free tier available
- **Speed**: Very fast
- **Quality**: Excellent
- **Setup**: `export GROQ_API_KEY=your-key`
- **Sign up**: https://console.groq.com

#### OpenAI / Anthropic
- **RAM**: < 1GB (runs in cloud)
- **Cost**: Paid (pay per use)
- **Quality**: Excellent
- **Setup**: Set API key environment variable

## ğŸ§ª Testing

### Test Retrieval-Only Mode

```bash
python test_system.py
```

Shows how the semantic search retrieves relevant chunks.

### Test with LLM

```bash
python test_with_llm.py
```

Downloads TinyLlama (first run) and generates answers.

## ğŸ”§ Configuration

Edit `config.py` or pass parameters to `FlexibleRAG`:

```python
from rag_flexible import FlexibleRAG

rag = FlexibleRAG(
    backend="retrieval_only",     # or "tinyllama", "phi2", "groq", etc.
    data_dir="./data",
    chunk_size=500,                # Characters per chunk
    chunk_overlap=50,              # Overlap between chunks
    k_retrieve=3                   # Number of chunks to retrieve
)

rag.setup()
result = rag.query("Your question here")
```

## ğŸ“Š Understanding Parameters

### Chunk Size & Overlap

- **chunk_size=500**: Size of text chunks in characters
  - Smaller (200-300): More precise retrieval
  - Larger (800-1000): More context per chunk

- **chunk_overlap=50**: Overlap between consecutive chunks
  - Ensures context isn't lost at chunk boundaries

### K (Retrieval Count)

- **k_retrieve=3**: Number of chunks to retrieve
  - Lower (1-2): Faster, more focused
  - Higher (5-10): More comprehensive, may include noise

## ğŸ› Common Issues & Solutions

### "Out of Memory" Error

**Solution**: Use retrieval-only mode or cloud API:
```python
backend = "retrieval_only"  # or "groq"
```

### "No documents found"

**Check**:
- Files are in `data/` folder
- Files are `.pdf` or `.txt` format
- Files are not empty

### API Key Not Found

**Set environment variable**:
```bash
# Linux/Mac/WSL
export GROQ_API_KEY="your-key-here"

# Windows (PowerShell)
$env:GROQ_API_KEY="your-key-here"
```

### Model Download is Slow

- This is normal! Models are 2-5GB
- Downloads only happen once
- Subsequent runs are fast (model is cached)

## ğŸ“ Learning Objectives

By completing this module, you will understand:

- âœ… What RAG is and when to use it
- âœ… How semantic search works with embeddings
- âœ… Document chunking strategies and trade-offs
- âœ… Retrieval quality vs quantity
- âœ… Local vs cloud LLM backends
- âœ… Real-world RAG challenges (retrieval accuracy, hallucination)

## ğŸ”¬ Experiments to Try

### 1. Compare Backends

Try the same question with different backends:
- Retrieval-only (see what chunks are found)
- TinyLlama (small model behavior)
- Groq (high-quality cloud model)

Compare answer quality, speed, and whether answers stay grounded in context!

### 2. Adjust Chunk Size

```python
# Try different chunk sizes
for size in [200, 500, 1000]:
    rag = FlexibleRAG(chunk_size=size)
    # Test the same question
```

### 3. Increase K (Retrieval Count)

```python
# Retrieve more chunks
rag = FlexibleRAG(k_retrieve=5)
```

Does more context help or hurt answer quality?

### 4. Test Retrieval Quality

Use retrieval-only mode to see what chunks are found. Are they relevant? If not, try:
- Different chunk sizes
- More overlap
- Better phrased questions

## ğŸ’¡ RAG vs Fine-tuning

| Aspect | RAG | Fine-tuning |
|--------|-----|-------------|
| **Setup Time** | Minutes | Hours/Days |
| **Data Required** | Documents | Labeled examples |
| **Updates** | Add docs instantly | Retrain model |
| **Cost** | Low | High (training compute) |
| **Use Case** | Q&A, knowledge base | Task adaptation, style |
| **Hallucination** | Lower (grounded) | Higher |

**Use RAG when:**
- You have a knowledge base or documents
- Information changes frequently
- You need source citations
- Quick prototyping

**Use Fine-tuning when:**
- Adapting model behavior or style
- No documents available at runtime
- Specific task performance
- Consistent format requirements

## ğŸ“š Additional Resources

- [SETUP_GUIDE.md](SETUP_GUIDE.md) - Detailed setup for Windows/Linux
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Guide](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

## ğŸ™ Acknowledgments

Built as part of the NVIDIA Ambassador Lab program, designed to teach RAG fundamentals with accessibility in mind - works on any hardware!

---

**Ready to learn more?** Check out the next module on fine-tuning to see an alternative approach to customizing LLMs!
